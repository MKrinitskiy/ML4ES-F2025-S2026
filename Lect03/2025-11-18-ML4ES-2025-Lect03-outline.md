# ML4ES 2025: Лекция 3

## Типы задач машинного обучения: постановка, структура и свойства

### 1. Введение и напоминание контекста курса
- Напоминание того, чем занимается курс ML4ES: машинное обучение, специализированное для наук о Земле.  
- Краткое повторение прошлых лекций: определения ИИ, связь между AI/ML/DL, роль исследователя.  
- Основные элементы постановки ML‑задачи: объекты/события, признаки, целевая переменная, мера качества.

---

### 2. Постановка задачи и компоненты ML‑модели
- Признаковое описание объектов: скалярное и векторное; физический смысл в задачах Земных наук.  
- Целевая переменная: скалярная или векторная; действительные значения vs категориальные.  
- Мера качества: определяется **до** начала решения задачи; критерий корректности и применимости модели.  
- Что означает «обобщающая модель»; концепция достоверности и применимости в задачах прогноза/аппроксимации.

---

### 3. Контролируемое обучение (Supervised Learning)
#### 3.1. Регрессия  
- Целевая переменная — действительное число (или вектор действительных чисел).  
- Иллюстрация: аппроксимация истинной зависимости, наличие шума измерений.  
- Переобучение: низкая способность к обобщению при идеальном попадании в обучающие точки.  
- Тестовая vs тренировочная ошибка, интерпретация разницы.

#### 3.2. Классификация  
- Целевая переменная — категория (метка класса).  
- Категориальная переменная: отсутствие отношения порядка между значениями.  
- Пример: бинарная классификация (true/false; красный/синий).  
- Оценка вероятности принадлежности классу в точке признакового пространства.  
- Поверхность принятия решения; вероятностная интерпретация.

---

### 4. Основные понятия вероятности и случайности в ML
- Почему признаки и целевые переменные — случайные величины.  
- Распределения, реализация случайных величин, роль теории вероятностей.  
- Для обучения и применения модели важна статистическая природа данных.

---

### 5. Тривиальные модели
- Константная модель (регрессия/классификация).  
- Рандомная модель.  
- Обсуждение: формально их можно рассматривать как baseline‑решения.

---

### 6. Неконтролируемое обучение (Unsupervised Learning)

#### 6.1. Кластеризация  
- Суть: поиск структуры в данных без известных меток.  
- Цель — присвоить объектам метки групп; иногда применять разбиение к новым данным.  
- Примеры методов: упоминание DBSCAN.  
- Важный тезис: **нет единственно правильного решения** кластеризации.  
- Два тривиальных решения:  
  - все объекты — один кластер;  
  - каждый объект — отдельный кластер.  
- Визуальные иллюстрации: простые случаи и сложные случаи с пересекающимися структурами.  
- Как интерпретировать роль новых признаков для улучшения структуры групп.  
- Ограничения визуализации при высокой размерности пространства.

#### 6.2. Задачи кластеризации в Земных науках  
- Пример: кластеризация состояний атмосферы в Карском/Баренцевом морях.  
- Кластеризация как инструмент разведочного анализа данных (EDA).  
- Интерпретация новых групп и связь с физическими процессами.

---

### 7. Проклятие размерности
- Пример: признаковое описание с огромным числом признаков (временные ряды × множество переменных).  
- Евклидово расстояние перестаёт отражать близость объектов в высоких размерностях.  
- Метрика Минковского как частичное решение и её ограничения.  
- Общий вывод: необходимость перехода к методам снижения размерности.

---

### 8. Снижение размерности  
- Идея: отобразить данные из пространства высокой размерности в пространство низкой (обычно 2–3).  
- Зачем необходимо:  
  - визуализация;  
  - снижение вычислительных затрат;  
  - борьба с шумом;  
  - борьба с проклятием размерности.  
- Требования к отображению:  
  - сохранение локальной близости;  
  - сохранение глобальных отношений, если возможно;  
  - определение близости «во времени» как пример физически осмысленной метрики.  
- Ограничения: любое снижение размерности = потеря информации.

---

### 9. Аппроксимация распределения данных  
- Мотивация: мало данных → нужно порождать дополнительные реалистичные примеры.  
- Порождающие модели:  
  - не выдают форму плотности распределения;  
  - но позволяют генерировать новые объекты, похожие на реальные.  
- Пример: генерация температур, состояния атмосферы, временных эволюций.  
- Принцип: плотность распределения выше → новая выборка чаще попадает туда.  
- Модели порождения — отдельный класс (в отличие от дискриминативных).

---

### 10. Самоконтролируемое обучение (Self‑supervised Learning)
- Идея: способ контроля (обучающая информация) находится внутри данных.  
- Связь с задачами реконструкции, заполнения пропусков, предсказания маскированных элементов.  
- Пример связи с задачей Оли (часть работы по состоянию атмосферы).  
- Это промежуточный класс между supervised и unsupervised.

---

### 11. Обучение с подкреплением (Reinforcement Learning)
- Отличия: нет заранее подготовленной обучающей выборки.  
- Агент действует в среде, получает вознаграждение/наказание.  
- Применение обычно не в наук о Земле, но важно знать как тип задач.

---

### 12. Выучивание меры различия (Metric Learning)
- Частный случай self‑supervised задач.  
- Цель — научиться вычислять «расстояние» между объектами согласно скрытым свойствам.  
- Используется для построения новых метрик и подготовки данных для кластеризации/снижения размерности.
